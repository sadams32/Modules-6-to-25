---
title: "Module14"
format: html
editor: visual
---

```{r}
library(curl)

f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/zombies.csv")
z <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
class(z$gender) # ask class, if not factor need to factorize it
```

```{r}
summary(z$gender)
```

```{r}
plot(z$height ~ z$gender)
```

```{r}
m <- lm(data = z, height ~ gender)
summary(m)
```

The estimate for ùõΩ1 is reported as ‚ÄúgenderMale‚Äù and the value for that coefficient, 4.0349, is the estimated difference in mean height associated with being a male compared to a female. The regression equation is basically: height = 65.5888 + 4.0349 x gender, with males assigned a gender value of 1 and females of 0.

### Beccause "female" comes before "male" or "nonbinary", the baseline is considered to be female and so male and non-binary are factor levels?

### Basline is female do the mean height of females is about 65.5888 -> each of the categories (4.0349, 2.3143) is that much taller (+) or shorter (-) than males. So if change from female to nonbinary (example) you would go up about 2.3143 inches in height. We are only ever comparing the other categories to the baseline, never comparing categories to one another (ex. this does not compare male and non-binary)

### "Residual standard error: 3.822 on 997 degrees of freedom" -> df = 997 because we are taking away 3 factors of about 1000 datapoints

### Multiple R-squared:  0.2152,	Adjusted R-squared:  0.2136 -> these values show how much variance of height is explained by gender FOR THIS MODEL (tells you if your model is good as well)

In this case, the p value (Pr > |t|) associated with the t statistic for ùõΩ1 is extremely low, so we conclude that ‚Äúgender‚Äù has a significant effect on height.

```{r}
levels(z$gender)
```

We can easily relevel() what is the baseline group (this becomes much more useful as we get more categorical variables in our regressions). The result is very similar, but the sign of ùõΩ1 is changed.

```{r}
z$gender <- relevel(z$gender, ref = "Male")
m <- lm(data = z, height ~ gender)
summary(m)
```
### The last few lines are the same despite changing male to the baseline because nothing has really changed but the reference

The last line of the summary() output shows the results of the global test of significance of the regression model based on an F statistic compared to an F distribution with, in this case, 1 and 998 degrees of freedom.

```{r}
p <- 1 - pf(276.9, df1 = 1, df2 = 998)
p
```

We can extend this approach to the case where we have more than two categories for a variable‚Ä¶ in this case we need to dummy code our factor variable into multiple binary variables. R takes care of this for us automatically, but it is good to recognize the procedure.

Let‚Äôs explore this by recoding the variable ‚Äúmajor‚Äù into four levels. We first create a new variable name.

```{r}
z$occupation <- "temp"
```

```{r}
unique(z$major)
```
```{r}
levels(z$major)
```
### Better to bin these majors above because for each one you have to remove a degree of freedom, also not very useful looking in this form

```{r}
row(data.frame(levels(z$major)))
```

```{r}
z$occupation[row(data.frame(levels(z$major))) %in% c(1, 2, 3, 5, 6, 14, 15,
    16, 18, 21, 23)] <- "natural science"
z$occupation[row(data.frame(levels(z$major))) %in% c(7, 8, 12, 17, 19, 22)] <- "logistics"
z$occupation[row(data.frame(levels(z$major))) %in% c(4, 18, 20)] <- "engineering"
z$occupation[row(data.frame(levels(z$major))) %in% c(9, 10, 11, 13, 24, 25,
    26)] <- "other"
z$occupation <- as.factor(z$occupation)
levels(z$occupation)
```

```{r}
z$occupation <- relevel(z$occupation, ref = "natural science")
levels(z$occupation)
```

Again, we can plot our variable by group and run a multilevel linear regression. Each ùõΩ
 estimate reflects the difference from the estimated mean for the reference level. The lm() function also returns the results of the global significance test of our model.

```{r}
plot(data = z, zombies_killed ~ occupation)
```

```{r}
m <- lm(data = z, zombies_killed ~ occupation)
summary(m)
```

```{r}
p <- 1 - pf(0.526, df1 = 3, df2 = 996)  # F test
p
```
In this case, we see no significant effect of major on zombie killing proficiency.

# One-Way ANOVA -> ANOVA tells you there is a difference, but not what the difference is!

Regression with a single categorical predictor run as we have just done above is exactly equivalent to a ‚Äúone-way‚Äù or ‚Äúone-factor‚Äù analysis of variance, or ANOVA. That is, ANOVA is just one type of special case of least squares regression.

We can thus run an ANOVA with one line in R. Compare the results presented in the summary() output table from an ANOVA with that from the global test reported in summary() from lm()

```{r}
m <- aov(data = z, zombies_killed ~ occupation)
summary(m)
```
```{r}
# Plot in console to see
par(mfrow = c(2, 2))
plot(m)
```

In general, in ANOVA and simple regression using a single categorical variable, we aim to test the ùêª0 that the means of a variable of interest do not differ among groups, i.e., that ùúá1 = ùúá2 ‚Ä¶ ùúáùëò
 are all equal. This is an extension of our comparison of two means that we did with z and t tests.

The basic linear model formulation for ANOVA is: Y(i,j) = ùúá + ùõΩùëñXùëñ + ùúñùëñ,ùëó

where:
ùúá is the grand population mean

ùõΩùëñ is the deviation of the mean of treatment level ùëñ
 from the grand mean

ùúñùëñ,ùëó is error variance of individual points from the grand mean

The assumptions of ANOVA, similar to those of simple regression, are:

that samples are independent and identically distributed

that the residuals ùúñùëñ,ùëó are normally distributed

that within-group variances are similar across all groups (‚Äôhomoscedastic‚Äù)


The following assumption makes the interpretation of results from ANOVA more straightforward, but it is not strictly required: Our experiment/observations have a balanced design (i.e., an equal number of cases in all groups). If this is violated, it ceases to be true that the total SS of our dataset = within SS + between SS, and then the calculations of MSE and F and our associated p values would be off.

# Challenge 1

Load in the ‚Äúgibbon-femurs.csv‚Äù dataset, which contains the lengths, in centimeters, of the femurs of 400 juvenile, subadult, and adult individuals gibbons. Use both ANOVA and simple linear regession to examine the relationship between age and femur length. Before beginning, make sure that you check for normality of observations within each group.

Is the omnibus test of the relationship between age category and femur length significant? Are femur lengths significantly different for juveniles versus subadults? Subadults versus adults? Juveniles versus adults? HINT: to test these bivariate options, you will need to relevel() your factors for simple linear regression.

```{r}
library(curl)
library(dplyr)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/gibbon-femurs.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
d$age <- factor(d$age, levels = c("inf", "juv", "subadult", "adult"))  #this reorders the age levels so that they're in order
head(d)
```

```{r}
hist(d$femur.length)
```

```{r}
qqnorm(d$femur.length)
```
```{r}
plot(data = d, femur.length ~ age)  # boxplot with medians
means <- summarise(group_by(d, age), mean(femur.length))  # calculate average by group
points(1:4, means$`mean(femur.length)`, pch = 4, cex = 1.5)  # add means to plot
```

```{r}
sds <- summarise(group_by(d, age), sd(femur.length))
max(sds$`sd(femur.length)`)/min(sds$`sd(femur.length)`)  # check that variances are roughly equal (ratio of max/min is <2)
```

```{r}
means.centered <- d$femur.length - means[as.numeric(d$age), 2]  # subtract relevant group mean from each data point
qqnorm(means.centered$`mean(femur.length)`)  # graphical tests for normality
```

```{r}
# Plot in console
par(mfrow = c(2, 2))
hist(d$femur.length[d$age == "inf"], main = "inf")
qqnorm(d$femur.length[d$age == "inf"])
hist(d$femur.length[d$age == "juv"], main = "juv")
qqnorm(d$femur.length[d$age == "juv"])
```

```{r}
hist(d$femur.length[d$age == "subadult"], main = "subadult")
qqnorm(d$femur.length[d$age == "subadult"])
hist(d$femur.length[d$age == "adult"], main = "adult")
qqnorm(d$femur.length[d$age == "adult"])
```

All this checking done, we can plot our data by group and run our ANOVA model‚Ä¶

```{r}
par(mfrow = c(1, 1))
plot(data = d, femur.length ~ age)
```
```{r}
m <- aov(data = d, femur.length ~ age)  # femur length related to age
summary(m)
```

```{r}
m <- lm(data = d, femur.length ~ age)
summary(m)
```

# Post-Hoc Tests and the Non-Parametric Kruskal-Wallis Test

After finding a significant omnibus F statistic in an ANOVA, we can test, post-hoc, what group means are different from one another using pairwise t tests with appropriate p value correction.


```{r}
pairwise.t.test(d$femur.length, d$age, p.adj = "bonferroni")
```
After an ANOVA, we can also use a ‚ÄúTukey Honest Significant Differences‚Äù test to evaluate this. -> get differences and confidence interval which is nice; see pairwise differences
```{r}
m <- aov(d$femur.length ~ d$age)
posthoc <- TukeyHSD(m, "d$age", conf.level = 0.95)
posthoc  # all age-sex classes differ
```

The Kruskal-Wallis test is a nonparametric alternative to one-way ANOVA that relaxes the need for normality in the distribution of data in each group (the different groups should still have roughly equal variances, though). Essentially, rather than testing the null hypothesis that the means for each group do not differ we are instead testing the null hypothesis that the medians do not differ. The test converts the continuous response variable to a set of RANKS (i.e., it does a uniform transformation) and then works with those ranks. The p value associated with the K-W test statistic is evaluated against a Chi-Square distribution. ->> need more significant difference in results becuase this test is much more conservative in establishing significance because this does NOT assume normality

```{r}
m <- kruskal.test(data = d, femur.length ~ age)
m
```

```{r}
d <- arrange(d, femur.length)  # use {dplyr} to sort by femur.length
d <- mutate(d, femur.rank = row(data.frame(d$femur.length)))  # use {dplyr} to add new variable of rank femur.length
m <- kruskal.test(data = d, femur.rank ~ age)
m
```
# Multiple Factor ANOVA

Sometimes the data we are interested in is characterized by multiple grouping variables (e.g., age and sex). In the case of the gibbon femur length data, we are interested in the main effect of each factor on the variable of interest (e.g., do femur lengths vary by age or sex) while accounting for the effects of the other factor. We may also be interested in any interactive effects among factors. Thus, in multiple factor ANOVA we are interested in testing several null hypotheses simultaneously: [1] that each factor has no effect on the mean of our continuous reponse variable and [2] that there are no interactive effects of sets of factors on the mean of our continuous reponse variable.

Model description and testing for multiple ANOVA is a simple extension of the formula notation which we‚Äôve used for single factors. First, though, let‚Äôs quickly check that our groups have similar variance.

```{r}
library(ggplot2)
means <- summarise(group_by(d, age, sex), mean(femur.length))  # first we calculate averages by combination of factors
```

```{r}
means
```

```{r}
sds <- summarise(group_by(d, age, sex), sd(femur.length))  # first we calculate averages by combination of factors
```

```{r}
sds
```

```{r}
max(sds$`sd(femur.length)`)/min(sds$`sd(femur.length)`)  # check that variances in each group are roughly equal (ratio of max/min is <2)
```

```{r}
p <- ggplot(data = d, aes(y = femur.length, x = sex)) + geom_boxplot() + facet_wrap(~age,
    ncol = 4)  # and let's plot what the data look like
# p <- p + geom_point() # uncommenting this shows all points
p <- p + stat_summary(fun.y = mean, colour = "darkgreen", geom = "point", shape = 8,
    size = 6)
p
```

If we look at each variable separately using ANOVA, we see there is an effect of age but not of sex

```{r}
summary(aov(data = d, femur.length ~ age))
```

```{r}
summary(aov(data = d, femur.length ~ sex))
```

However, if we do a two-way ANOVA and consider the factors together, we see that there is still a main effect of age when taking sex into account and there is a main effect of sex when we take age into account.
```{r}
m <- summary(aov(data = d, femur.length ~ age + sex))
m
```
### The + in "age+sex" does not mean we are looking at interactions between them, just means we are looking at both -> might sitll be missing those interactions!

To examine whether there is an interaction effect, we would modify our model formula a bit‚Ä¶

```{r}
m <- aov(data = d, femur.length ~ age + sex + age:sex)  # : operator includes specific interaction terms
summary(m)
```
We could also use‚Ä¶
```{r}
m <- aov(data = d, femur.length ~ age * sex)  # * operator includes all interaction terms
summary(m)
```
```{r}
m <- lm(data = d, femur.length ~ age * sex)  # or using the lm() function...
summary(m)
```

```{r}
interaction.plot(x.factor = d$age, xlab = "Age", trace.factor = d$sex, trace.label = "Sex",
    response = d$femur.length, fun = mean, ylab = "Mean Femuur Length")
```
Here, it looks like there is indeed a significant main effect of each term as well as an interaction between our two categorical variables. We will return to comparing models to one another (e.g., our model with and without interaction) and to post-hoc tests of what group mean differences are significant when we get into model selection in another few lectures.

When we do summary() of the results of the lm() function, we are estimating eight ùõΩ coefficients (equivalent to the number of groups we have). ùõΩ0, the intercept, which is the mean femur length for the base level (in this case, ‚Äúadult females‚Äù). Then we have coefficients showing how the different factor combination groups would differ from that base level (e.g., adult males have mean femur lengths 1.716 greater than adult females, etc).

Note that the ORDER in which our factors are entered into our linear model results in different values for the entries in our ANOVA table, while the estimation of our regression coefficients is identical regardless:
```{r}
m1 <- aov(data = d, femur.length ~ age * sex)
summary(m1)
```

```{r}
m2 <- aov(data = d, femur.length ~ sex * age)
summary(m2)
```

```{r}
m1 <- lm(data = d, femur.length ~ age * sex)
summary(m1)
```

```{r}
m2 <- lm(data = d, femur.length ~ sex * age)
summary(m2)
```
Why is this? In the first case, we are looking at the variance within each age group that is explained by gender while in the second case we are looking at the variance within each gender that is explained by age‚Ä¶ but we have different numbers of observations in our different groups. This is known as an unbalanced design.

We can see the unbalanced design by tabulating the cases for each combination of factors.
```{r}
table(d$sex, d$age)
```
By default, the aov() function uses something called Type I Sums of Squares (also called ‚Äúsequential sum of squares‚Äù), which gives greater emphasis to the first factor in the model, leaving only residual variation to the remaining factors. It should be used when you want to first control for one factor, leaving the others to explain only any remaining differences. In a Type I ANOVA, the sums of squares for the first term are calculated around the grand mean of our observations, but the next terms are calculated as residuals around the average of the grand mean and the first group mean. This sequential procedure means our results depend on which term shows up first. This can have a large effect on the sums of squares calculated, and hence on p values, when using an unbalanced design.

By contrast, Type II and Type III Sums of Squares calculate individual observations‚Äô within each group as deviations from the grand mean.

By contrast, Type II Sum of Squares compares the main effects of each group, assuming that the interaction between them is minimal. Generally it is a much more appropriate test for comparing main effects and is more appropriate for us to use when there is an unbalanced design.

Type III Sum of Squares (or ‚Äúmarginal sum of squares‚Äù) is most appropriate when there is a significant interaction effect. Since both Type II and Type III ANOVA calculate sums of squares around the grand mean, these are unaffected by sample sizes and do not arbitrarily give preference to one effect over another.

To summarize:

When our data are balanced, our factors are orthogonal, and all three types of sums of squares give the same results.

If our data are unbalanced, we should generally be using Type II or Type III Sums of Squares, since we are generally interested in exploring the significance of one factor while controlling for the level of the other factors.

In general, if there is no significant interaction effect, then Type II is more powerful.

If interaction is present, then Type II is inappropriate while Type III can still be used, but results need to be interpreted with caution (in the presence of interactions, main effects difficult to interpret).

See this post for further treatment of this issue.

We can use the Anova() function in the {car} package to run ANOVA with Type II and Type III Sums of Squares. In the examples below, both linear models, where the order of factors are reversed, give the same results.

```{r}
library(car)
```

```{r}
m1 <- aov(data = d, femur.length ~ age + sex)
m1 <- Anova(m1, type = "II")
m1
```
```{r}
m1 <- aov(data = d, femur.length ~ sex + age)
m2 <- Anova(m2, type = "II")
m2
```

```{r}
m1 <- aov(data = d, femur.length ~ age * sex)
m1 <- Anova(m1, type = "III")
m1
```

```{r}
m2 <- aov(data = d, femur.length ~ sex * age)
m2 <- Anova(m2, type = "III")
m2
```
# Chi-Square Tests of Goodness of Fit and Independence
One additional type of categorical data we will often encounter are counts of observations that fall into two or more categories (when we were dealing with Z tests for proportion data, we were interested in something similar, though with two categories only). We can use Chi-Square tests to evaluate statistically the distribution of observations across levels of one or more categorical variables. To use the Chi-Square test we first derive a Chi-Square statistic, which is calculated as‚Ä¶

where: [look online]

ùëÇùëñ = number of observations in the ùëñth category
ùê∏ùëñ = number of observations in the ùëñth category
We then compare the value of the ùúí2 statistic to the Chi-Square distribution with ùëò‚àí1 degrees of freedom.

# Challenge 2

Let‚Äôs return to the zombies dataset, where we defined an occupation based on major for survivors of the zombie apocalypse. We want to test the hypothesis that survivors of the zombie apocalypse are more likely than expected by chance to be natural science majors. We will assume that our null hypothesis is that the proportions of different post-apocalypse occupations are equivalent, i.e., that ùúã(natrual science) = ùúãengineering = ùúãlogistics = ùúãother = 0.25.

```{r}
obs.table <- table(z$occupation)  # returns the same as summary()
obs.table
```

```{r}
exp.table <- rep(0.25 * length(z$occupation), 4)
exp.table
```

```{r}
occupation.matrix <- data.frame(cbind(obs.table, exp.table, (obs.table - exp.table)^2/exp.table))
names(occupation.matrix) <- c("Oi", "Ei", "(Oi-Ei)^2/Ei")
occupation.matrix
```

```{r}
X2 <- sum(occupation.matrix[, 3])
X2
```

```{r}
p <- 1 - pchisq(X2, length(obs.table) - 1)
p
```

Here, we reject the null hypothesis that the proportions of different occupations among the survivors of the zombie apocalypse is equivalent.

We can do all this with a 1-liner in R, too.

```{r}
chisq.test(x = obs.table, p = c(0.25, 0.25, 0.25, 0.25))  # here p is a vector of expected proportions... default is uniform
```

```{r}
chisq.test(x = obs.table)
```

```{r}
chisq.test(x = obs.table, p = c(0.38, 0.12, 0.23, 0.27))  # with a different set of expected proportions... fail to reject H0
```
The above was a Chi-Square goodness of fit test for one categorical variable‚Ä¶ what about if we have two categorical variables and we are curious if there is an association among them? Then we do a Chi-Square test of independence. In this case, our Chi-Square statistic is the sum of (O-E)^2/E across all cells in our table, and our degrees of freedom is = (number of rows - 1) * (number of columns -1). Let‚Äôs suppose we want to see if there is a relationship among zombie apocalypse survivors between gender and occupation.

First, we determine our table of observed proportions:
```{r}
obs.table = table(z$gender, z$occupation)
obs.table
```
We can view our data graphically using the mosaic plot() function.

```{r}
mosaicplot(t(obs.table), main = "Contingency Table", col = c("darkseagreen",
    "gray"))  # t function transposes the table
```
Then, we determine our table of expected proportions:
```{r}
r <- rowSums(obs.table)  # row margins
r
```

```{r}
c <- colSums(obs.table)  # column margins
c
```
```{r}
nr <- nrow(obs.table)  # row dimensions
nr
```

```{r}
nc <- ncol(obs.table)  # column dimensions
nc
```

```{r}
exp.table <- matrix(rep(c, each = nr) * r/sum(obs.table), nrow = nr, ncol = nc,
    dimnames = dimnames(obs.table))  # calculates the product of c*r and divides by total
exp.table
```

```{r}
X2 <- sum((obs.table - exp.table)^2/exp.table)
X2
```

```{r}
p <- 1 - pchisq(X2, df = (nr - 1) * (nc - 1))
p
```

Again, we can do a one-liner for a test of independence‚Ä¶

```{r}
chisq.test(x = obs.table)
```


