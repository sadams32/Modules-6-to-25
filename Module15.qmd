---
title: "Module15"
format: html
editor: visual
---

```{r}
library(curl)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(car)
```


ANOVA = "analysis of covariance"
: used when our predictors are combinations of continuous and categorical variables

When we do multiple linear regression, we are, essentially, looking at the relationship between each of two or more continuous predictor variables and a continuous response variable while holding the effect of all other predictor variables constant. When we do ANCOVA, we are effectively looking at the relationship between one or more continuous predictor variables and a continuous response variable within each of one or more categorical groups.

The criteria we typically use for estimating best fit is analogous to that we‚Äôve used before, i.e., ordinary least squares, where we want to minimize the multidimensional squared deviation of our observed and predicted values

# Multiple Regression - Continuous Response Variable and More than One Continuous Predictor Variables

We will start by constructing a dataset ourselves of some correlated random normal continuous variables. First, we define a matrix of correlations among our variables (you can play with the values in this matrix, but it must be symmetric):

```{r}
R = matrix(cbind(1, 0.8, -0.5, 0, 0.8, 1, -0.3, 0.3, -0.5, -0.3, 1, 0.6, 0,
    0.3, 0.6, 1), nrow = 4)
```

Second, let‚Äôs generate a dataset of random normal variables where each has a defined mean and standard deviation and then bundle these into a matrix (‚ÄúM‚Äù) and a dataframe (‚Äúorig‚Äù):

```{r}
n <- 1000
k <- 4
M <- NULL
V <- NULL
mu <- c(15, 40, 5, 23)  # vector of variable means
s <- c(5, 20, 4, 15)  # vector of variable SDs
for (i in 1:k) {
    V <- rnorm(n, mu[i], s[i])
    M <- cbind(M, V)
}
M <- matrix(M, nrow = n, ncol = k)
orig <- as.data.frame(M)
names(orig) = c("Y", "X1", "X2", "X3")
head(orig)
```

```{r}
cor(orig)  # variables are uncorrelated
```

```{r}
plot(orig)  # does quick bivariate plots for each pair of variables; using `pairs(orig)` would do the same
```

Now, let‚Äôs normalize and standardize our variables by subtracting the relevant means and dividing by the standard deviation. This converts them to Z scores from a standard normal distribution.

```{r}
ms <- apply(orig, 2, FUN = "mean")  # returns a vector of means, where we are taking this across dimension 2 of the array 'orig'
ms
```

```{r}
sds <- apply(orig, 2, FUN = "sd")
sds
```

```{r}
normalized <- sweep(orig, 2, STATS = ms, FUN = "-")  # 2nd dimension is columns, removing array of means, function = subtract
normalized <- sweep(normalized, 2, STATS = sds, FUN = "/")  # 2nd dimension is columns, scaling by array of sds, function = divide
head(normalized)  # now a dataframe of Z scores
```

```{r}
M <- as.matrix(normalized)  # redefine M as our matrix of normalized variables
```

With apply() we apply a function to the specified margin of an array or matrix, and with sweep() we then perform whatever function is specified on all of the elements in an array specified by the given margin.

Next, we take the Cholesky decomposition of our correlation matrix and then multiply our normalized data matrix by the decomposition matrix to yield a transformed dataset with the specified correlation among variables. The Cholesky decomposition breaks certain symmetric matrices into two such that:

```{r}
U = chol(R)
newM = M %*% U
new = as.data.frame(newM)
names(new) = c("Y", "X1", "X2", "X3")
cor(new)  # note that is correlation matrix is what we are aiming for!
```
```{r}
plot(orig)
```

```{r}
plot(new)  # note the axis scales; using `pairs(new)` would plot the same
```

Finally, we can scale these back out to the mean and distribution of our original random variables.

```{r}
df <- sweep(new, 2, STATS = sds, FUN = "*")  # scale back out to original mean...
df <- sweep(df, 2, STATS = ms, FUN = "+")  # and standard deviation
head(df)
```
Question from me: are we supposed to get different values from this ^?

```{r}
cor(df)
```
```{r}
plot(df)  # note the change to the axis scales; using `pairs(d)` would produce the same plot
```
Now, we have a dataframe, df, comprising correlated random variables in our original units!

Let‚Äôs explore this dataset first with single and then with multivariate regression.

# Challenge 1

implemented with lm(), how does the response variable (ùëå) vary with each predictor variable (ùëã1, ùëã2, ùëã3)? Are the ùõΩ1 coefficients significant? How much of the variation in ùëå does each predictor explain in a simple bivariate linear model?

```{r}
g1 <- ggplot(data = df, aes(x = X1, y = Y)) + geom_point() + geom_smooth(method = "lm",
    formula = y ~ x)
g2 <- ggplot(data = df, aes(x = X2, y = Y)) + geom_point() + geom_smooth(method = "lm",
    formula = y ~ x)
g3 <- ggplot(data = df, aes(x = X3, y = Y)) + geom_point() + geom_smooth(method = "lm",
    formula = y ~ x)
grid.arrange(g1, g2, g3, ncol = 3)
```

```{r}
m1 <- lm(data = df, formula = Y ~ X1)
summary(m1)
```

```{r}
m2 <- lm(data = df, formula = Y ~ X2)
summary(m2)
```

```{r}
m3 <- lm(data = df, formula = Y ~ X3)
summary(m3)
```

In simple linear regression, ùëå has a significant, positive relationship with ùëã1, a signficant negative relationship with ùëã2, and no significant bivariate relationship with ùëã3.

Now let‚Äôs move on to doing actual multiple regression. To review, with multiple regression, we are looking to model a response variable in terms of two or more predictor variables so we can evaluate the effect of several explanatory variables.

Using lm() and formula notation, we can fit a model with all three predictor variables. The + sign is used to add additional predictors to our model.

```{r}
m <- lm(data = df, formula = Y ~ X1 + X2 + X3)
coef(m)
```

```{r}
summary(m)
```

```{r}
# let's check if our residuals are random normal...
plot(fitted(m), residuals(m))
```

```{r}
hist(residuals(m))
```

```{r}
qqnorm(residuals(m))
```

What does this output tell us? First off, the results of the omnibus F test tells us that the overall model is significant; using these three variables, we explain signficantly more of the variation in the response variable, Y, than we would using a model with just an intercept, i.e., just that ùëå = mean(ùëå).

For a multiple regression model, we calculate the F statistic as follows (LOOK UP EQN)
Where:
ùëÖ2 = multiple R squared value

ùëõ = number of data points

ùëù = number of parameters estimated from the data (i.e., the number of ùõΩ coefficients, not including the intercept)

```{r}
f <- (summary(m)$r.squared * (nrow(df) - (ncol(df) - 1) - 1))/((1 - summary(m)$r.squared) *
    (ncol(df) - 1))
f
```

Second, looking at summary() we see that the ùõΩ coefficient for each of our predictor variables (including ùëã3) is significant. That is, each predictor is significant even when the effects of the other predictor variables are held constant. Recall that in the simple linear regression, the ùõΩ coefficient for ùëã3 was not significant.

Third, we can interpret our ùõΩ coefficients as we did in simple linear regression‚Ä¶ for each change of one unit in a particular predictor variable (holding the other predictors constant), our predicted value of the response variable changes ùõΩ units.


# Challenege 2

Load up the ‚Äúzombies.csv‚Äù dataset again and run a linear model of height as a function of both weight and age. Is the overall model significant? Are both predictor variables significant when the other is controlled for?

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/zombies.csv")
z <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(z)
```


```{r}
m <- lm(data = z, height ~ weight + age)
summary(m)
```

All of the p-values are significant (***) and the model as a whole is also significant (F-statistic p-value < 2.2e-16)

# ANCOVA - Continuous Response Variable and Both Continuous and Categorical Predictor Variables

We can use the same linear modeling approach to do analysis of covariance, where we have a continuous response variable and a combination of continuous and categorical predictor variables. Let‚Äôs return to our ‚Äúzombies.csv‚Äù dataset and now include one continuous and one categorical variable in our model‚Ä¶ we want to predict height as a function of age and gender, and we want to use Type II regression. What is our model formula?

```{r}
m <- lm(data = z, formula = height ~ gender + age)
summary(m)
```

```{r}
m.aov <- Anova(m, type = "II")
```

```{r}
plot(fitted(m), residuals(m))
```

```{r}
hist(residuals(m))
```

```{r}
qqnorm(residuals(m))
```
How do we interpret these results?

The omnibus F test is significant

Both predictors are significant

Controlling for age, being male adds 4 inches to predicted height when compared to being female

## Visualizing the Model

We can write two equations for the relationship between height on the one hand and age and gender on the other:

For females, height = 46.7251 + 0.94091 x age
In this case, females are the first level of our ‚Äúgender‚Äù factor and thus do not have additional regression coefficients associated with them.

For males, height = 46.7251 + 4.00224 + 0.94091 x age
Here, the additional 4.00224 added to the intercept term is the coefficient associated with genderMale

```{r}
p <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +
    scale_color_manual(values = c("goldenrod", "blue"))
p <- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1],
    color = "goldenrod4")
p <- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1] +
    m$coefficients[2], color = "darkblue")
p
```
Note that this model is based on all of data collectively‚Ä¶ we are not doing separate linear models for males and females, which could result in different slopes and intercepts for each sex. Below, we will explore a model where we posit an interaction between age and sex, which would require estimation of four separate parameters (i.e., both a slope and an intercept for males and females rather than, as above, different intercepts more males and females but the same slope for each sex).

## Confidence Intervals and Prediction

Using the confint() function on our ANCOVA model results reveals the confidence intervals for each of the coefficients in our multiple regression, just as it did for simple regression.

```{r}
m <- lm(data = z, formula = height ~ age + gender)
summary(m)
```

```{r}
confint(m, level = 0.95)
```
Similarly, using predict() allows us to determine confidence intervals for the predicted mean response and prediction intervals for individual responses for a given combination of predictor variables.

## Challenge 3

What is the estimated mean height, in inches, for a 29 year old male who has survived the zombie apocalypse?

What is the 95% confidence interval around this mean height?

```{r}
ci <- predict(m, newdata = data.frame(age = 29, gender = "Male"), interval = "confidence",
    level = 0.95)
ci
```


What is the 95% prediction interval for the individual heights of 29 year old male survivors?

```{r}
pi <- predict(m, newdata = data.frame(age = 29, gender = "Male"), interval = "prediction",
    level = 0.95)
pi
```

## Interactions Between Predictors

So far, we have only considered the joint main effects of multiple predictors on a response variable, but often there are interactive effects between our predictors. An interactive effect is an additional change in the response that occurs because of particular combinations of predictors or because the relationship of one continuous variable to a response is contingent on a particular level of a categorical variable. We explored the former case a bit when we looked at ANOVAs involving two discrete predictors. Now, we‚Äôll consider the latter case‚Ä¶ is there an interactive effect of sex AND age on height in our population of zombie apocalypse survivors?

Using formula notation, it is easy for us to consider interactions between predictors. The colon (:) operator allows us to specify particular interactions we want to consider; we can use the asterisk (*) operator to specify a full model, i.e., all single terms factors and their interactions.

```{r}
m <- lm(data = z, height ~ age + gender + age:gender)  # or
summary(m)
```

```{r}
m <- lm(data = z, height ~ age * gender)
summary(m)
```

```{r}
coefficients(m)
```
Here, when we allow an interaction, there is no main effect of gender, but there is an interaction effect of gender and age.

If we want to visualize this‚Ä¶

female height = 48.1817041 + 0.8891281 * age

male height = 0.481704 + 1.1597481 + 0.8891281 * age + 0.1417928 * age

```{r}
p1 <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +
    scale_color_manual(values = c("goldenrod", "blue"))
p1 <- p1 + geom_abline(slope = m$coefficients[2], intercept = m$coefficients[1],
    color = "goldenrod4")
p1 <- p1 + geom_abline(slope = m$coefficients[2] + m$coefficients[4], intercept = m$coefficients[1] +
    m$coefficients[3], color = "darkblue")
p1
```

```{r}
p2 <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +
    scale_color_manual(values = c("goldenrod", "blue")) + geom_smooth(method = "lm",
    aes(color = factor(gender), fullrange = TRUE))
grid.arrange(p1, p2, ncol = 2)
```

## Challenge 4

Load in the ‚ÄúKamilarAndCooper.csv‚Äù" dataset we have used previously

Reduce the dataset to the following variables: Family, Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, DayLength_km, HomeRange_km2, and Move

Fit a Model I least squares multiple linear regression model using log(HomeRange_km2) as the response variable and log(Body_mass_female_mean), log(Brain_Size_Female_Mean), MeanGroupSize, and Move as predictor variables, and view a model summary.

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
```

```{r}
d <- select(d, Brain_Size_Female_Mean, Family, Body_mass_female_mean, MeanGroupSize,
    DayLength_km, HomeRange_km2, Move)
```

Look at and interpret the estimated regression coefficients for the fitted model and interpret. Are any of them statistically significant? What can you infer about the relationship between the response and predictors?

Report and interpret the coefficient of determination and the outcome of the omnibus F test.

Examine the residuals‚Ä¶ are they normally distributed?

What happens if you remove the ‚ÄúMove‚Äù term?

```{r}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean) + log(Brain_Size_Female_Mean) +
    MeanGroupSize + Move)
summary(m)
```

```{r}
plot(m$residuals)
```

```{r}
qqnorm(m$residuals)
```

```{r}
shapiro.test(m$residuals)
```

```{r}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean) + log(Brain_Size_Female_Mean) +
    MeanGroupSize)
summary(m)
```

```{r}
plot(m$residuals)
```

```{r}
qqnorm(m$residuals)
```

```{r}
shapiro.test(m$residuals)  # no significant deviation from normal
```