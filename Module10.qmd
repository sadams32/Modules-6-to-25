---
title: "Module10"
format: html
editor: visual
---

# Testing Sample Means: One Sample Z and T Tests

Letâ€™s do an example where we try to evaluate whether the mean of a single set of observations is significantly different than expected under a null hypothesisâ€¦ i.e., this is a ONE-SAMPLE test.

Suppose we have a vector describing the adult weights of vervet monkeys trapped in South Africa during the 2016 trapping season. We have the sense they are heavier than vervets we trapped in previous years, which averaged 4.9 kilograms. The mean is 5.324 kilograms. Is the mean significantly greater than our expectation?

```{r}
library(curl)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/vervet-weights.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

```{r}
mean(d$weight)
```

```{r}
mu <- 4.9
x <- d$weight
m <- mean(x)
s <- sd(x)
n <- length(x)
sem <- s/sqrt(n)
```

```{r}
z <- (m - mu)/sem
z
```

n this case, ğ‘§
 is a quantileâ€¦ the estimated number of standard errors of the mean away from the population mean that our sample falls.

If we then want to see if ğ‘§
 is significant, we need to calculate the probability of seeing a deviation from the mean as high or higher than this by chance. To do this, we can use the pnorm() function. Because we have converted our sample mean to the standard normal scale, the mean= and sd= arguments of pnorm() are the defaults of 0 and 1, respectively.

We want the probability of seeing a ğ‘§
 this large or larger by chance.
 
```{r}
p <- 1 - pnorm(z)
p
```
 
```{r}
p <- pnorm(z, lower.tail = FALSE)
p
```


However, as noted above, our sample size from a population is typically limited. So, instead of using the normal distribution to determine the p value of our statistic, we should use the t distribution, which, as weâ€™ve seen, has slightly fatter tails. The statistic and process is exactly the same, though, as for the normal distribution.

```{r}
p <- 1 - pt(z, df = n - 1)
p
```

```{r}
p <- pt(z, df = n - 1, lower.tail = FALSE)
p
```

R has built into it a single function, t.test(), that lets us do all this in one line. We give it our data and the expected population mean, ğœ‡
, along with the kind of test we want to do.

```{r}
t <- t.test(x = x, mu = mu, alternative = "greater")
t
```

Note that we can also use the t.test() function to calculate CIs based on the t distribution for us easily!

```{r}
lower <- m - qt(1 - 0.05/2, df = n - 1) * sem
upper <- m + qt(1 - 0.05/2, df = n - 1) * sem
ci <- c(lower, upper)
ci  # by hand
```

```{r}
t <- t.test(x = x, mu = mu, alternative = "two.sided")
ci <- t$conf.int
ci  # using t test
```

So our conclusion, then, would be to reject the ğ»0
 that the weights of the sample of monkeys from the 2016 trapping season come from the same distribution as the weights of monkeys from previous trapping seasons based on the average, since the average from previous seasons falls outside the 95% confidence interval for the t distribution based on the sample average from the 2016 trapping season.

In other words, the 2016 trapping season has vervet monkeys whose weights are significantly heavier than the average from previous trapping seasons (p < 0.01).

## Challenge 1

Adult lowland woolly monkeys are reported to have an average body weight of 7.2 kilograms. You are working with an isolated population of woolly monkeys from the Peruvian Andes that you think may be a different species from the lowland form, and you collect a sample of 15 weights from adult individuals at that site. From your sample, you calculate a mean of 6.43 kilograms and a standard deviation of 0.98 kilograms.

Perform a hypothesis test of whether body weights in your population are different from the reported average for lowland woolly monkeys by setting up a â€œtwo-tailedâ€ hypothesis, carrying out the analysis, and interpreting the p value (assume the significance level is ğ›¼
=0.05). Your sample is < 30, so you should use the t distribution and do a t test. Do your calculations both by hand and using the t.test() function and confirm that they match.

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/woolly-weights.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

```{r}
x <- d$weight
m <- mean(x)
s <- sd(x)
n <- length(x)
sem <- s/sqrt(n)
mu <- 7.2
t <- (m - mu)/sem
t
```

```{r}
alpha <- 0.05
crit <- qt(1 - alpha/2, df = n - 1)  # identify critical values
test <- t < -crit || t > crit  # boolean test as to whether t is larger than the critical value at either tail
test <- abs(t) > crit
t.test(x = x, mu = mu, alternative = "two.sided")
```


# Comparing Sample Means: Two Sample Z and T Tests

Sometimes we want to compare two groups of measurements to one another, which boils down to a hypothesis test for the difference between two means, ğœ‡1
 and ğœ‡2
. The null hypothesis is that the difference between the means is zero.

Before getting to the appropriate test, there are a couple of things that we need to consider:

[1] How, if at all, are the two samples related to one another? Sometimes we may have PAIRED samples (e.g., the same individuals before and after some treatment) and sometimes the samples are UNPAIRED or INDEPENDENT (e.g., weights for different samples of black-and-white colobus monkeys collected in the rainy versus dry seasons).

[2] Are the variances in the two samples roughly equal or not? E.g., if we are comparing male and female heights, are the variances comparable?

## Samples with Unequal Variances
### Challenge 2
Letâ€™s do an example. Load in a file of black-and-white colobus weights and examine the str() of the file. Then, create 2 vectors, x and y, for male and female weights. Plot these in boxplots side by side and then calculate the mean, sd, and sample size for both males and females.

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/colobus-weights.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

```{r}
x <- d$weight[d$sex == "male"]
y <- d$weight[d$sex == "female"]
par(mfrow = c(1, 2))
boxplot(x, ylim = c(4.5, 8), main = "Weight (kg)", xlab = "Males")
boxplot(y, ylim = c(4.5, 8), main = "Weight (kg)", xlab = "Females")
```

```{r}
m1 <- mean(x)
m2 <- mean(y)
mu <- 0  # you could leave this out... the default argument value is 0
s1 <- sd(x)
s2 <- sd(y)
n1 <- length(x)
n2 <- length(y)
```

Now calculate the t statistic and test the two-tailed hypothesis that the sample means differ.

```{r}
t <- (m2 - m1 - mu)/sqrt(s2^2/n2 + s1^2/n1)
t
```

```{r}
alpha <- 0.05
crit <- qt(1 - alpha/2, df = n - 1)  # identify critical values
crit
```

```{r}
test <- t < -crit || t > crit  # boolean test
test <- abs(t) > crit
test
```

Note that for this test, the number of degrees of freedom is calculated as:

```{r}
df <- (s2^2/n2 + s1^2/n1)^2/((s2^2/n2)^2/(n2 - 1) + (s1^2/n1)^2/(n1 - 1))
df
```
Do the same using the t.test() function
```{r}
t <- t.test(x = x, y = y, mu = 0, alternative = "two.sided")
t
```

## Samples with Equal Variances

The degrees of freedom for this test is calculated as follows:
```{r}
s <- sqrt((((n1 - 1) * s1^2) + ((n2 - 1) * s2^2))/(n1 + n2 - 2))
t <- (m2 - m1 - mu)/(sqrt(s^2 * (1/n1 + 1/n2)))
t
```

```{r}
df <- n1 + n2 - 2
df
```

```{r}
t <- t.test(x = x, y = y, mu = 0, var.equal = TRUE, alternative = "two.sided")
t
```
A crude test for equality of variances is to divide the larger by the smaller and if the result is < 2, you can go ahead and used the pooled variance version of the test (which has many fewer degrees of freedom).

In our case, we cannot, since the ratio of variances exceeds 2â€¦
```{r}
var(x)/var(y)
```

We can use the var.test() function to conduct an actual statistical test on the ratio of variances, which compares the ratio test statistic we just calculated to an F distribution. The F distribution is often used to model ratios of random variables and thus is useful in regression applications and, as here, for testing whether variances from two samples are different. It is dependent upon the specification of a pair of degrees of freedom values supplied as the arguments df1= and df2= (or inferred from the number of observations in each sample).

Below, the results of var.test() are saved to a variable. Calling the variable provides a brief descriptive summary.
```{r}
vt <- var.test(x, y)
vt
```

## Paired Samples

For a paired sample test, the null hypothesis is that the mean of individual paired differences between the two samples (e.g., before and after) is zero.

### Challenge 3
Letâ€™s play with a sampleâ€¦ IQs of individuals taking a certain statistics course before and after a lecture on significance testing. Load in the iqs.csv data file, look at it, plot a barchart of values before and after and construct a paired t test to evaluate the means before and after.

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/iqs.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

```{r}
x <- d$IQ.before - d$IQ.after
m <- mean(x)
mu <- 0  # can leave this out
s <- sd(x)
n <- length(x)
sem <- s/sqrt(n)
par(mfrow = c(1, 2))
boxplot(d$IQ.before, ylim = c(115, 145), main = "IQ", xlab = "Before")
boxplot(d$IQ.after, ylim = c(115, 145), main = "IQ", xlab = "After")
```

```{r}
t <- (m - mu)/sem
t
```

```{r}
alpha <- 0.05
crit <- qt(1 - alpha/2, df = n - 1)  # identify critical values
crit
```

```{r}
test <- t < -crit || t > crit  # boolean test
test
```

```{r}
t.test(x, df = n - 1, alternative = "two.sided")
```

# Testing Sample Proportions: One Sample Z Test

As we have seen, the sampling distribution of sample means for independent and identically distributed random variables is roughly normal (and, as shown by the CLT, this distribution increasingly approaches normal as sample size increases). Similarly, the sampling distribution for another kind of sample statistic, the number of â€œsuccessesâ€ x out of a series of k trials is also roughly normally distributed. If the true population proportion of â€œsuccessesâ€ is ğœ‹, then the sampling distribution for the proportion of successes in a sample of size n is expected to be roughly normally distributed with mean = ğœ‹and standard error = sqrt(ğœ‹(1âˆ’ğœ‹)/ğ‘›).

Letâ€™s set up a simulation to show thisâ€¦

First we create a population of 500 â€œ1â€s and 500 â€œ0â€s, i.e., where ğœ‹ = 0.5

```{r}
pop <- c(rep(0, 500), rep(1, 500))
```

Now, we will take 1000 random samples of size n=10 from that population and calculate the proportion of â€œ1â€s in each sampleâ€¦
```{r}
pi <- 0.5
x <- NULL
n <- 10
for (i in 1:1000) {
    x[i] <- mean(sample(pop, size = n, replace = FALSE))  # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!
}
m <- mean(x)
m
```

```{r}
s <- sd(x)
s
```

```{r}
pop_se <- sqrt(pi * (1 - pi)/n)
pop_se  # the se is an estimate of the sd of the sampling distribution
```

The same is true if we create a population of 800 â€œ1â€s and 200 â€œ0â€s, i.e., where ğœ‹ = 0.8â€¦
```{r}
pop <- c(rep(0, 800), rep(1, 200))
pi <- 0.8
x <- NULL
n <- 10
for (i in 1:1000) {
    x[i] <- mean(sample(pop, size = n, replace = FALSE))  # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!
}
m <- mean(x)
m
```

```{r}
s <- sd(x)
s
```

```{r}
pop_se <- sqrt(pi * (1 - pi)/n)
pop_se  # the se is an estimate of the sd of the sampling distribution
```

z = (observed statistic - expected statistic) / standard error (see equation in notes)

### Challenge 4

A neotropical ornithologist working in the western Amazon deploys 30 mist nets in a 100 hectare (ha) grid. She monitors the nets on one morning and records whether or not she captures any birds in the net (i.e., a â€œsuccessâ€ or â€œfailureâ€ for every net during a netting session). The following vector summarizes her netting results:
```{r}
v <- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1)
```

Her netting success over the previous three seasons suggests that she should catch birds in 80% of her nets. This season, she feels, her success rate is lower than in previous years. Does her trapping data support this hypothesis?

What is ğ»0?
What is ğ»A?
Are both nÃ—ğœ‹and ğ‘›Ã—(1âˆ’ğœ‹) > 5?
Calculate z and the p value associated with z
Calculate the 95% CI around p-hat

```{r}
phat <- mean(v)
phat
```

```{r}
pi <- 0.8
n <- 30
z <- (phat - pi)/sqrt(pi * (1 - pi)/30)
z
```

```{r}
p <- pnorm(z, lower.tail = TRUE)
p
```

We use the lower.tail=TRUE argument because weâ€™re testing a lower-tailed one-tailed hypothesis. The 95% confidence interval can be estimated, based on the normal distribution, as follows:
```{r}
lower <- phat - qnorm(0.975) * sqrt(phat * (1 - phat)/30)
upper <- phat + qnorm(0.975) * sqrt(phat * (1 - phat)/30)
ci <- c(lower, upper)
ci
```


This approach using quantiles of the standard normal distribution is but one method of calculating CIs for proportion data, and is the CI referred to as a Wald confidence interval. Note that the CI does not include the value of ğœ‹â€¦ rather, ğœ‹ > is greater than the upper bound of the CI, suggesting that the observed success rate is indeed lower than in previous years.

We can do the same test with a one-liner in Râ€¦
```{r}
pt <- prop.test(x = sum(v), n = length(v), p = 0.8, conf.level = 0.95, correct = FALSE,
    alternative = "less")
pt
```

Note that the CI is different than we calculated based on the normal distributionâ€¦ prop.test() implements a slightly different procedure for estimating the CI rather than basing this on the normal distribution and the CLT.

# Comparing Sample Proportions: Two Sample Z Tests

### Challenge 5

A biologist studying two species of tropical bats captures females of both species in a mist net over the course of week of nightly netting. For each species, the researcher records whether females are lactating or not. The two vectors below summarize the data for each species.
```{r}
v1 <- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0)
v2 <- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1)
```

Based on your mist netting data, do the species differ significantly in the proportion of lactating females during this trapping season? What are ğ»0 and ğ»A?

```{r}
pstar <- (sum(v1) + sum(v2))/(length(v1) + length(v2))
pstar
```

```{r}
phat1 <- mean(v1)
phat1
```

```{r}
phat2 <- mean(v2)
phat2
```

```{r}
pi <- 0
z <- (phat2 - phat1)/sqrt((pstar * (1 - pstar)) * (1/length(v1) + 1/length(v2)))
z
```

```{r}
p <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
p
```

```{r}
crit <- qnorm(1 - alpha/2)  # identify critical values
crit
```

```{r}
test <- p < -crit || p > crit  # boolean test
test
```

We can use the prop.test() function to do this in one line.
```{r}
pt <- prop.test(x = c(sum(v2), sum(v1)), n = c(length(v2), length(v1)), alternative = "two.sided",
    correct = FALSE)
pt
```

See notes for summary of Z and T Tests
